%!TEX root = main.tex

MTL has become a provalent tool in dealing with the problem of learning from the distributed data in the epoch of big data. In spite of its proliferation, challenges also arise. One indispensable challenge is about how the privacy of sensitive data is protected in distributed MTL framework. For example, medical centers in different countries may make a joint effort to conduct medical research, while the data may not be distributed because of its sensitive nature.

On sight, it seems that erasing personal information such as names can protect individual privacy. However, as the development of machine learning algorithms, it is still possible to extract patterns from remaining information and obtain personal information. Back to 2007, the famous media service provider Netflix published an anonymous dataset called Netflix Prize dataset, and encouraged researchers to design a better recommendation system. The dataset contains 10 million movie rankings from 500,000 customers. However, even customers' personal details like usernames and locations are removed and replaced by random numbers, some information is still deanonymized by comparing rankings and timestamps with public resources from the Internet Movie Database (IMDb)~\cite{4531148}. Other works also demonstrate it. \cite{doi:10.1111/j.1748-720X.1997.tb01885.x} and \cite{DBLP:journals/corr/abs-0803-0032} can extract hidden information from an adversary. Even genetic datasets can leak personal information\cite{10.1371/journal.pgen.1000167,Wang:2009:LYI:1653662.1653726}.

% \cite{Backstrom:2007:WAT:1242572.1242598} proves that a family under attack can be performed to social network nodes anonymized. 

As early as 1977, \cite{dalenius1977towards} has defined a concept for the ideal database: nothing about an individual that can not be learned without the database should be learned from the database. Although the idea is too demanding and is later demonstrated theoretically that this desideratum can not be reached\cite{Dwork06differentialprivacy}, effort can still be done to reduce the risk of leaking personal information stored in the dataset. In a word, differential privacy aims at maximizing the accuracy of queries to a database while minimizing the possibility of identifying a single record.

In this paper, we proposed a new method to address the issue arising from Smith {\it et al.}~\cite{smith2017interaction}. In~\cite{smith2017interaction}, the authors provided the answer to the question: "how much interaction is necessary to optimize convex functions in the local DP model?". Although this non-interactive setting achieves good performance, the testing result under the scenario of distributed MTL with trustworthy aggregator is far from satisfying. This is due to the fact that multitask learning aims at improving each user's performance with the help from all the others, which requires a lot of information exchange. Also in real world cases, heterogeneity of local data and tasks make it very hard to finish training in a few rounds. In addition, a method with few interactive or non-interactive typically requires a large amount of local datasets. This is hard to achieve due to the scarcity of the data under distributed MTL senses. 

In this paper, we will show that, with a trustworthy data aggregator, asynchronous interaction performs better than the proposed one in~\cite{smith2017interaction} . In addition, \cite{smith2017interaction} mentioned that it is difficult to implement an interactive framework for private data learning because of two reasons: (1) long network latency; (2) the server has to be online for asynchronous updates. The first issue can be addressed with the asynchronous update, whose effect will be further reduced with weighted update mechanism we proposed. The second issue can be addressed by data backup, which is easy to achieve because the complexity of the algorithm in aggregator is moderate.

In summary, this paper makes the following contributions:
\begin{itemize}
  \item We present the first distributed private learning system with fast light-weight interactions under the condition of a trustworthy central data processor.
  \item We carefully design the noise perturbation algorithm that is added on the aggregator and prove that the proposed algorithm guarantees local differential privacy (LDF)~\cite{duchi2013local}--one of the most important variants of DP. We check the correctness of our method using state-of-the-art tool~\cite{ding2018toward}.
  \item We reduce the accumulation effect of privacy leakage during iterative updates. We also explore the effect of different distributions of the delay and use weights to eliminate the effect of the delay as well as exploring other delay-reduce methods.
  \item We demonstrate our method with sufficient empirical evidences, which contain multiple real world settings such as task heterogeneity and data heterogeneity, 
\end{itemize}

The paper is arranged as follows: we first introduce the related work in section 2 with respect to differential privacy and distributed rrivate learning. Next in section 3, we give a detailed description on our model and method. Experiments results are provided in section 4, as well as analysis. We also discuss our technical Weaknesses and potential future work in section 5. We conclude our work in section 6 and provide details on project group members in section 7.
% The lower complexity than~\cite{smith2017interaction}
% The results are better than the non-interactive one presented in ~\cite{smith2017interaction}
