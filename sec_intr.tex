%!TEX root = main.tex

MTL has become an indispensable tool in dealing with the problem of learning from distributed data in the epoch of big data. One major challenge arises when using MTL. The challenge is that how the privacy of the sensitive data is protected in distributed MTL framework. For example, medical centers in
different countries may make a joint effort to conduct medical research, while the data may not be distributed because of its sensitive nature.

On sight, it seems that erasing personal information such as names can protect individual's privacy. However, as the development of machine learning algorithms, it's still possible to extract patterns from remaining information and obtain personal information. For example, in 2007, the famous media service provider Netflix published an anonymous dataset called Netflix Prize dataset and encouraged researchers to design a better recommendation system. The dataset contains 10 million movie rankings from 500,000 customers. However, even customers' personal details are removed and replaced by random numbers, some customers are still deanonymized by comparing rankings and timestamps with public information in the Internet Movie Database (IMDb)~\cite{4531148}. Other works also demonstrate it. \cite{doi:10.1111/j.1748-720X.1997.tb01885.x} and \cite{DBLP:journals/corr/abs-0803-0032} can extract hidden information from an adversary. Even genetic datasets can leak personal information\cite{10.1371/journal.pgen.1000167,Wang:2009:LYI:1653662.1653726}.

% \cite{Backstrom:2007:WAT:1242572.1242598} proves that a family under attack can be performed to social network nodes anonymized. 

As early as 1977, \cite{dalenius1977towards} has defined a concept for ideal database: nothing about an individual that can't be learned without the database should be learned from the database. However, it's later demonstrated theoretically that this desideratum can't be reached\cite{Dwork06differentialprivacy}. But still, effort can be done to reduce the risk of leaking personal information stored in the dataset. So in general, differential privacy aims at maximizing the accuracy of queries to a database while minimizing the possibility of identifying a single record.

In this paper, we proposed a new method to address the issue arising from Smith {\it et al.}~\cite{smith2017interaction}. In~\cite{smith2017interaction}, the authors provided the answer to the question: "how much interaction is necessary to optimize convex functions in the local DP model?". However, this non-interactive setting does not provide a good performance under the scenario of distributed MTL with trustworthy aggregator. This is due to the fact that multitask learning aims at improving each user's performance with the help from all the others, which requires a lot of information exchange. Also in real world cases, heterogeneity of local data and tasks made it very hard to finish training in a few rounds. In addition, method with few interactive or non-interactive typically requires a large amount of local dataset. This is hard to achieve due to the scarcity of the data under distributed MTL sense. 

We will show that with a trustworthy data aggregator, asynchronous interaction performs better than the proposed one in~\cite{smith2017interaction} . In addition, \cite{smith2017interaction} mentioned that it is difficult to implement interactive framework for private data learning because of two reasons: (1) long network latency. (2) the server has to be online for asynchronous updates. The 1st issue can be addressed with the asynchronous update, whose effect will be further reduced with weighted update mechanism we proposed. The second issue can be addressed by data backup, which is easy to achieve because the complexity of the algorithm in aggregator is moderate.

In summary, this paper makes the following contributions:
\begin{itemize}
  \item we present the first distributed private Learning system with fast light-weight interaction under the condition of trustworthy central data processor.
  \item we carefully design the noise perturbation algorithm that is added on the aggregator and proves that the proposed algorithm guarantees local differential privacy (LDF)~\cite{duchi2013local}--one of the most important variants of DP. We check the correctness of our method using state-of-the-art tool~\cite{ding2018toward}.
  \item we reduce the accumulation effect of privacy leakage during iterative updates. We also explore the effect of different distributions of delay and use weight to eliminate the effect of delay and explore other delay-reduce methods.
  \item we demonstrate our method with sufficient empirical evidences, which contain multiple real world settings such as task heterogeneity and data heterogeneity, 
\end{itemize}

% The lower complexity than~\cite{smith2017interaction}
% The results are better than the non-interactive one presented in ~\cite{smith2017interaction}
