%!TEX root = main.tex

\textbf{Differential privacy.} Enormous algorithms have been proposed for privacy-preserving data mining\cite{Agrawal:2000:PDM:335191.335438,Evfimievski:2003:LPB:773153.773174,Sweeney:2002:KAM:774544.774552,Machanavajjhala:2007:LDP:1217299.1217302}. But composition attacks and auxiliary information become big problem for these algorithms. On the other hand, differential privacy acts as an resistance against composition attacks and auxiliary information.

When differential privacy is first put forward by C.Dwork\cite{Dwork:2006:CNS:2180286.2180305}, a sensitivity method is also introduced. If we denote the objective function as $J$ and the true query result coming from algorithm $\mathcal{A}$ is represented by $\mathcal{A}(D) = argmin\ J $, the output query result is $\mathcal{A}(D)+b$ where $b$ is an random noise with density $\frac{1}{\alpha} e^{-\beta \norm{b}}$. $\beta$ is a function of $\epsilon$ and the $L_2$-sensitivity of $\mathcal{A}(\cdot)$. The sensitivity method is a typical output perturbation method.

As for the objective perturbation methods, the objective function is turned to be $J(f,D)+\frac{1}{n}b^T f$ where $f$ is the predictor and $n$ is the number of training data points. If the objective function is strongly convex with some constraints on the loss function, objective perturbation proves theoretically better than output perturbation. More details can be found in \cite{Chaudhuri:2011:DPE:1953048.2021036}.

\textbf{Distributed Private Learning.} 
There are also important studies on distributed differentially private Learning. Xie {\it et al.}~\cite{xie2017privacy} for the first time, provide a privacy-preserving distributed MTL framework combined with distributed asynchronous MTL framework. The proposed method successfully address the issue of time delay caused by synchronized optimization algorithms. It is different from our settings because it assumes that the central server is untrustworthy and adding noise from local task side may significantly reduce the overall performance. Xie {\it et al.}~\cite{xie2016data} proposed a ensemble learning method for merging binary classifiers (or regressors) trained on local data. This method, though can be applied in our paper's setting with ''public-private" case, does not help much due to data heterogeneity. In~\cite{xie2016comparison} the authors proved that the method~\cite{xie2016data} has near-optimal performance under certian conditions. Han {\it et al.}~\cite{han2017differentially} present a distributed optimization algorithm (with constrained domain) that preserves differential privacy. This method may not achieve good performance due to its exponential mechanism. Rajkumar and Agarwal~\cite{rajkumar2012differentially} describe a new differentially private algorithm for the multiparty setting that uses a stochastic gradient descent based procedure to directly optimize the overall multiparty objective. This paper does not address the issue of accumulation effect of privacy leakage. Hamm {\it et al.}~\cite{hamm2016learning} proposed a method of building a global
differentially private classifier from locally classifiers from multiple local users without access to their private data. Similar as~\cite{xie2016data}, it does not help due to data heterogeneity. Pathak {\it et al.}~\cite{pathak2010multiparty} proposed a privacy-preserving framework for composing a differentially private aggregate
classifier using local trained classifiers by separate mutually untrusting
parties. This method requires other encryption method, which may lead to more cost.

